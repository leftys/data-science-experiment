{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory (in MB) = 49.733632\n",
      "After imports memory (in MB) = 122.273792\n",
      "With numpy arrays memory = 682.84416\n",
      "After constructing lgb.Dataset and freeing numpy memory = 281.919488\n",
      "After training memory = 282.84928\n",
      "Peak memory (when both numpy and Dataset are allocated) memory =  861.036\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print('Initial memory (in MB) =', process.memory_info().rss / 1e6)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import resource\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print('After imports memory (in MB) =', process.memory_info().rss / 1e6)\n",
    "\n",
    "# Should take up 7*10MB*4 (7 columns of 10 million numbers in 8 byte float64 format) = 560 MB\n",
    "y_train = np.random.random(10_000_000)\n",
    "X_train = pd.DataFrame(\n",
    "    data = np.random.random((10_000_000, 6)),\n",
    "    # If more separete numpy arrays are used, lightgbm performs extra data copy and uses twice more memory.\n",
    "    #'col1': np.random.random(10_000_000),\n",
    "    #'col2': np.random.random(10_000_000),\n",
    "    columns = ['col'+str(i) for i in range(6)],\n",
    ")\n",
    "\n",
    "\n",
    "print('With numpy arrays memory =', process.memory_info().rss / 1e6)\n",
    "\n",
    "# Create dataset for lightgbm. \n",
    "# The dataset uses less memory than numpy, as features are somehow compressed/bucketed by quantlies or something :D. \n",
    "# `construct` actually performs the copy and compression, before that, no memory is initialized.\n",
    "lgb_train = lgb.Dataset(X_train.values, y_train).construct()\n",
    "del X_train, y_train\n",
    "\n",
    "print('After constructing lgb.Dataset and freeing numpy memory =', process.memory_info().rss / 1e6)\n",
    "# 300 MB on my PC means that the dataset takes around 120MB which means \n",
    "# features are probably bucketed by quantiles into 255 buckets taking 1 byte of memory per number (7x smaller) \n",
    "# and labels are kept as precise 8 byte floats.\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 2,\n",
    "}\n",
    "gbm = lgb.train(params,\n",
    "    lgb_train,\n",
    "    num_boost_round = 1,\n",
    ")\n",
    "\n",
    "print('After training memory =', process.memory_info().rss / 1e6)\n",
    "print('Peak memory (when both numpy and Dataset are allocated) memory = ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
